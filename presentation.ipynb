{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spaCy, wersja NASK\n",
    "7 kwietnia 2022\n",
    "\n",
    "Model oparty o Herberta w wersji base (https://huggingface.co/allegro/herbert-base-cased).\n",
    "Trenowany na NKJP 1M po konwersji do UD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ładowanie modelu, wersja spacy, i wyniki."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"pl_nask\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"spaCy version:\", spacy.__version__)\n",
    "performance_data = nlp.meta[\"performance\"]\n",
    "acc_keys = [\"tag_acc\", \"pos_acc\", \"morph_acc\", \"dep_uas\", \"dep_las\", \"ents_f\", \"ents_r\", \"ents_p\"]\n",
    "print(\"\\nEvaluation results:\")\n",
    "for key in acc_keys:\n",
    "    print(key, round(performance_data[key] * 100, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Skład pipeline'u:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, component in nlp.pipeline:\n",
    "    print(name, component)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Anotacja morfologiczna i leksykalna\n",
    "\n",
    "Informacja o tagach morfologicznych wygląda nastepująco:\n",
    "\n",
    "    tok.tag_ \n",
    "zawiera pełen tag morfologiczny w tagsecie SGJP\n",
    "\n",
    "    tok.pos_\n",
    "zawiera klasę części mowy w tagsecie UPOS\n",
    "\n",
    "    tok.morph\n",
    "zawiera cechy morfologiczne w tagsecie UFEATS\n",
    "\n",
    "    tok.lemma_\n",
    "zawiera lemat pochodzący z analiz Morfeusza, dezambiguowanych przez tok.tag_ oraz dane frekwencyjne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "\n",
    "def table_tags(doc):\n",
    "    tok_dicts = []\n",
    "    for tok in doc:\n",
    "        tok_dict = {\n",
    "                    \"orth\": tok.orth_,\n",
    "                    \"lemma\": tok.lemma_,\n",
    "                    \"UPOS\": tok.pos_,\n",
    "                    \"SGJP\": tok.tag_,}\n",
    "        tok_dicts.append(tok_dict)\n",
    "    return pandas.DataFrame(tok_dicts)\n",
    "\n",
    "def table_morphs(doc):\n",
    "    tok_dicts = []\n",
    "    for tok in doc:\n",
    "        tok_dict = {\n",
    "                    \"orth\": tok.orth_,\n",
    "                    \"lemma\": tok.lemma_,\n",
    "                    \"UFEAT\": str(tok.morph)[:40]}\n",
    "        tok_dicts.append(tok_dict)\n",
    "    return pandas.DataFrame(tok_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \"Ani skuteczna dyplomacja, ani moralne wsparcie, ani profetyczne gesty. \\\n",
    "Tak podsumować można linię Stolicy Apostolskiej w czasie wojny w Ukrainie. \\\n",
    "Rosyjska agresja boleśnie zweryfikowała teologię pokoju i wizję geopolityczną papieża.\"\n",
    "doc = nlp(txt)\n",
    "print(table_tags(doc), \"\\n\\n\")\n",
    "print(table_morphs(doc), \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obsługujemy prostą, słownikową dedyminutywizację: Z SGJP i Wiktionary zebraliśmy ponad 6 tys. par zdrobnienie - forma bazowa, które wykorzystujemy, słownik ten można rozbudowywać, znajduje się w plikach modelu.\n",
    "\n",
    "    tok._.is_diminutive\n",
    "wartość logiczna opisująca czy dany token wyraża zdrobnienie\n",
    "\n",
    "    tok._.diminutive_chain\n",
    "lista zdrobnień w łańcuchu aż do ostatniej formy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \"Spotkałem się z Alą, żeby porozmawiać o jej milutkim synku - Piotrusiu. Ona jest wspaniałą mamusią.\"\n",
    "doc2 = nlp(txt)\n",
    "for tok in doc2:\n",
    "    print(tok.i, tok.orth_, tok.lemma_, tok._.is_diminutive, tok._.diminutive_chain)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Morfeusz zwraca również kilka kwalifikatorów o charakterze leksykalnym, zapisujemy je w rozszerzonych atrybutach:\n",
    "\n",
    "    tok._.properness\n",
    "lista kwalifikatorów opisujących pospolitość/własność rzeczowników.\n",
    "\n",
    "    tok._.disambiguator\n",
    "\"rozpodabniacz\", rozróżniający różne leksemy, o identycznie brzmiących lematach, ale różnych wzorcach odmiany.\n",
    "\n",
    "    tok._.style\n",
    "lista kwalifikatorów dotyczących m.in. nacechowania stylistycznego.\n",
    "\n",
    "    tok._.is_ign\n",
    "atrybut określający, czy wskazane słowo jest poza słownikiem morfeusza, atrybut ten można wykorzystywać zamiast tok.is_oov, którego to nie można nadpisać, i jest normalnie ustawiane na podstawie embeddingów word2vec, których w tym modelu nie stosujemy\n",
    "\n",
    "    tok._.freq\n",
    "częstość bezwzględna danego lematu w NKJP1M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def table_additional_annotations(doc):\n",
    "    tok_dicts = []\n",
    "    for tok in doc:\n",
    "        tok_dict = {\n",
    "                    \"orth\": tok.orth_,\n",
    "                    \"properness\": tok._.properness,\n",
    "                    \"disambiguator\": tok._.disambiguator,\n",
    "                    \"style\": tok._.style,\n",
    "                    \"ign\": tok._.is_ign,\n",
    "                    \"freq\": tok._.freq\n",
    "        }\n",
    "        tok_dicts.append(tok_dict)\n",
    "    return pandas.DataFrame(tok_dicts)\n",
    "\n",
    "txt2 = \"A to zwykłe mendy, co im zawiniły firanki?!? Podaj mi francuzy, s'il vous plait\"\n",
    "doc2 = nlp(txt2)\n",
    "\n",
    "print(table_additional_annotations(doc2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regułowe wyszukiwanie wzorców\n",
    "\n",
    "(https://spacy.io/usage/rule-based-matching)\n",
    "\n",
    "SpaCy umożliwia wyszukiwanie w tekście wzorców. Wzorce opisują ciągłe sekwencje tokenów, w terminach udostępnianych przez SpaCy atrybutów tokenów. Liczba obsługiwanych atrybutów oraz ekspresywność warunków jakie możemy formułować przekracza zakres tej prezentacji, natomiast możemy wskazać kilka podstawowych funkcjonalności.\n",
    "\n",
    "Wzorce mogą być również definiowane w terminach UFEATS, wówczas porównujemy nie pojedyncze wartości, a słowniki wartości."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher\n",
    "\n",
    "pattern_fem_sing = [{\"MORPH\": {\"IS_SUPERSET\": [\"Number=Sing\", \"Gender=Fem\"]}}]\n",
    "\n",
    "matcher = Matcher(nlp.vocab, validate=True)\n",
    "matcher.add(\"FemSing\", [pattern_fem_sing])\n",
    "\n",
    "matches = matcher(doc)\n",
    "for match in matches:\n",
    "    rule_identifier, start, end = match\n",
    "    rule_name = nlp.vocab.strings[rule_identifier]\n",
    "    print(f\"{rule_name}: {doc[start:end]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher.remove(\"FemSing\")\n",
    "pattern_fem_sing_noun = [{\"MORPH\": {\"IS_SUPERSET\": [\"Number=Sing\", \"Gender=Fem\"]},\n",
    "                          \"POS\": \"NOUN\"}]\n",
    "matcher.add(\"FemSingNoun\", [pattern_fem_sing_noun])\n",
    "\n",
    "matches = matcher(doc)\n",
    "for match in matches:\n",
    "    rule_identifier, start, end = match\n",
    "    rule_name = nlp.vocab.strings[rule_identifier]\n",
    "    print(f\"{rule_name}: {doc[start:end]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher.remove(\"FemSingNoun\")\n",
    "pattern_fem_sing_adj_noun = [{\"MORPH\": {\"IS_SUPERSET\": [\"Number=Sing\", \"Gender=Fem\"]},#Pierwszy token\n",
    "                          \"POS\": \"ADJ\"},\n",
    "                         {\"MORPH\": {\"IS_SUPERSET\": [\"Number=Sing\", \"Gender=Fem\"]},# Drugi token\n",
    "                          \"POS\": \"NOUN\"}]\n",
    "matcher.add(\"FemSingAdjNoun\", [pattern_fem_sing_adj_noun])\n",
    "pattern_fem_sing_noun_adj = [{\"MORPH\": {\"IS_SUPERSET\": [\"Number=Sing\", \"Gender=Fem\"]},#Pierwszy token\n",
    "                          \"POS\": \"NOUN\"},\n",
    "                         {\"MORPH\": {\"IS_SUPERSET\": [\"Number=Sing\", \"Gender=Fem\"]},# Drugi token\n",
    "                          \"POS\": \"ADJ\"}]\n",
    "matcher.add(\"FemSingNounAdj\", [pattern_fem_sing_noun_adj])\n",
    "\n",
    "matches = matcher(doc)\n",
    "for match in matches:\n",
    "    rule_identifier, start, end = match\n",
    "    rule_name = nlp.vocab.strings[rule_identifier]\n",
    "    print(f\"{rule_name}: {doc[start:end]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsowanie zależnościowe\n",
    "Parser wytrenowano na NKJP 1M w wersji UD, zgodnym z anotacją PDB. Dane są częściowo anotowane ręcznie, częściowo automatycznie (COMBO)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "displacy.render(doc, style='dep',jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regułowe wyszukiwanie wzorców w drzewach zależnościowych\n",
    "\n",
    "(https://spacy.io/usage/rule-based-matching)\n",
    "\n",
    "W języku polskim wymóg odgórnego określania kolejności tokenów, oraz ciągłości sekwencji może być mocno ograniczający. Możemy więc wykorzystać narzędzie o większej mocy, pozwalające definiować wzorce w terminach położenia w drzewie zależnościowym (które może być zgoła odmienne od horyzontalnego porządku tokenów w tekście)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import DependencyMatcher\n",
    "\n",
    "matcher = DependencyMatcher(nlp.vocab)\n",
    "\n",
    "dep_noun_adj = \\\n",
    "[\n",
    "    {\n",
    "        \"RIGHT_ID\": \"noun\",\n",
    "        \"RIGHT_ATTRS\": {\"POS\": \"NOUN\"}\n",
    "    },\n",
    "    {\n",
    "        \"LEFT_ID\": \"noun\",\n",
    "        \"REL_OP\": \">\",\n",
    "        \"RIGHT_ID\": \"modifier\",\n",
    "        \"RIGHT_ATTRS\": {\"DEP\": \"amod\"}\n",
    "    }\n",
    "]\n",
    "\n",
    "matcher.add(\"NounAdj\", [dep_noun_adj])\n",
    "matches = matcher(doc)\n",
    "for match in matches:\n",
    "    rule_identifier, matching_tokens = match\n",
    "    rule_name = nlp.vocab.strings[rule_identifier]\n",
    "    ordered_phrase = [doc[tok_i] for tok_i in sorted(matching_tokens)]\n",
    "    print(f\"{rule_name}: {ordered_phrase}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coordination = [{\n",
    "        \"RIGHT_ID\": \"subordinate\",\n",
    "        \"RIGHT_ATTRS\": {\"DEP\": \"conj\"}\n",
    "    },\n",
    "    {\n",
    "        \"LEFT_ID\": \"subordinate\",\n",
    "        \"REL_OP\": \"<\",\n",
    "        \"RIGHT_ID\": \"superior\",\n",
    "        \"RIGHT_ATTRS\": {}\n",
    "    },\n",
    "    {\n",
    "        \"LEFT_ID\": \"subordinate\",\n",
    "        \"REL_OP\": \">\",\n",
    "        \"RIGHT_ID\": \"cc\",\n",
    "        \"RIGHT_ATTRS\": {\"DEP\": {\"IN\": [\"cc\", \"cc:preconj\"]}}\n",
    "    }]\n",
    "    \n",
    "\n",
    "from spacy.matcher import DependencyMatcher\n",
    "matcher = DependencyMatcher(nlp.vocab)\n",
    "matcher.add(\"Coordination\", [coordination])\n",
    "matches = matcher(doc)\n",
    "for match in matches:\n",
    "    rule_identifier, matching_tokens = match\n",
    "    rule_name = nlp.vocab.strings[rule_identifier]\n",
    "    ordered_phrase = [doc[tok_i] for tok_i in sorted(matching_tokens)]\n",
    "    print(f\"{rule_name}: {ordered_phrase}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rozpoznawanie jednostek nazewniczych\n",
    "\n",
    "Model obsługuje 5 podstawowych kategorii opisanych w NKJP:\n",
    "PLACENAME, GEOGNAME, PERSNAME, TIME, DATE\n",
    "\n",
    "Nie obsługujemy jednostek zazębiających się (a więc również jednostek zagnieżdżonych)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(doc, style='ent',jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Przetwarzanie batchy tekstów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orchids =\"Storczykowate są rodziną kosmopolityczną, występującą na wszystkich kontynentach, z wyjątkiem Antarktydy. Największe zróżnicowanie gatunkowe storczyków występuje w strefie międzyzwrotnikowej, a zwłaszcza w tropikach na kontynentach amerykańskich i w Azji południowo-wschodniej po Nową Gwineę. W tropikach amerykańskich rośnie 350 rodzajów i ok. 10 tys. gatunków, tylko w Malezji jest ich 4,5 tys. gatunków, a na Nowej Gwinei 2,3 tys. Z tropikami związanych jest 36 najbardziej zróżnicowanych gatunkowo rodzajów, liczących ponad 100 gatunków, i tylko nieliczne z nich mają przedstawicieli poza strefą równikową.\"\n",
    "nettles = \"Pokrzywa – rodzaj jednorocznych roślin zielnych lub bylin z rodziny pokrzywowatych (Urticaceae Juss.). Należy do niej co najmniej 68 gatunków rozpowszechnionych na całej kuli ziemskiej z wyjątkiem Antarktydy. Rośliny niektórych gatunków dostarczają włókna i są jadalne. \"\n",
    "bridge = \"Brydż (ang. bridge) – logiczna gra karciana, w której bierze udział czterech graczy tworzących dwie rywalizujące ze sobą pary[2]. Gracze stanowiący parę siedzą naprzeciwko siebie. Każda para stara się uzyskać lepszy wynik punktowy od wyniku przeciwników. Gra składa się z dwóch odrębnych części: licytacji oraz rozgrywki. Podczas licytacji gracze deklarują wzięcie pewnej minimalnej liczby lew oraz wskazują kolor atutowy lub jego brak, a najwyższa deklaracja staje się kontraktem ostatecznym, z którego trzeba się wywiązać podczas drugiej części zwanej rozgrywką.\"\n",
    "jokers = \"Joker (wym. „dżoker” czyli ang. żartowniś), dżoker – jedna z kart do gry, w niektórych grach karcianych (na przykład kierki) służy do zastępowania dowolnej innej karty. W standardowej brydżowej talii kart znajdują się dwa lub trzy jokery oprócz 52 kart zwykłych. Najczęściej spotykanym wizerunkiem na jokerze jest kolorowo ubrany błazen (trefniś, ang. joker od joke – „żart”, z łac. iocus czytaj jokus) w czapce z dzwoneczkami. Jokery oprócz wizerunku błazna bywają oznaczane w narożniku karty gwiazdką lub (niekiedy, jeśli nie koliduje to z oznaczeniem waleta) literą J; spotykane są też inne oznaczenia, na przykład znakiem dolara.\"\n",
    "tanks = \"Pierwsze czołgi brytyjskie przypominały opancerzone skrzynie, opasane z dwóch stron metalowymi gąsienicami. Nowy rodzaj mechanizmu jezdnego umożliwiał pokonywanie trudnych przeszkód, w tym okopów, a także miażdżenie zasieków z drutu kolczastego. Pierwsze czołgi były maszynami bardzo prymitywnymi. Aby wykonać ostry skręt, wymagały skoordynowanej pracy czterech osób, co było nie lada osiągnięciem. Pojazd nie miał wentylacji, co powodowało, że gazy spalinowe i prochowe wywoływały często omdlenia i zatrucia załogi.\"\n",
    "pistol = \"Pistolet – krótka, ręczna broń palna (z wyłączeniem rewolwerów) zasilana najczęściej amunicją pistoletową, rzadziej rewolwerową (słabszą od karabinowej i pośredniej). Pistolety przeznaczone są do walki na krótkim dystansie (do 50 m). Charakteryzują się krótką lufą, małymi gabarytami i chwytem (rękojeścią) przystosowanym do strzelania z jednej ręki. Najpowszechniej stosowane w wojsku, policji i ochronie. Są także popularną bronią sportową. \"\n",
    "pope = \"Biskupi Rzymu oparli swój prymat na sukcesji apostolskiej, zgodnie z tradycją, według której pierwszym biskupem Rzymu był Piotr Apostoł, który zginął tam śmiercią męczeńską. Nowy Testament milczy wprawdzie na ten temat i wspomina tylko o podróży św. Piotra do Antiochii (Gal 2, 11), a w zakończeniu Listu do Rzymian Pawła Apostoła pośród licznych osób Piotr nie jest wymieniony, ale o pobycie apostoła w Rzymie mówią inne pisma z pierwszych wieków istnienia chrześcijaństwa, m.in. list biskupa Antiochii Ignacego do Kościoła w Rzymie, napisany za panowania cesarza Trajana (98–117).\"\n",
    "knights = \"Wpływ na powstanie tej grupy miały przemiany społeczno-polityczne na terenie dawnego imperium Karolingów. Wiązały się one z kryzysem władzy centralnej i kształtowaniem się stosunków zależności feudalnej. Równocześnie nastąpił wzrost znaczenia ciężkiej konnicy w prowadzeniu wojen, powodujące zapotrzebowanie na konnych wojowników.\"\n",
    "george = \"We w pełni rozwiniętej wersji zachodniej smok zrobił sobie gniazdo na źródle, którego woda zaopatrywała miasto Silene (prawdopodobnie późniejsza Cyrena w Libii) lub miasto Lod, zależnie od źródeł. W konsekwencji mieszkańcy musieli prosić smoka o opuszczenie gniazda na czas, gdy nabierali wodę. Każdego dnia oferowali mu owcę, a jeśli jej nie znaleźli, musieli oddać zamiast niej jedną dziewczynę. Ofiara była wybierana przez losowanie.\"\n",
    "greece = \"Grecja pozostaje pod wpływem klimatu śródziemnomorskiego. Cechuje go łagodna zima z suchym, gorącym latem. W najcieplejszym miesiącu średnia temperatura wynosi ponad 22 °C. Są co najmniej cztery miesiące ze średnią temperaturą ponad 10 °C, w zimie mogą zdarzać się przymrozki. Notuje się co najmniej trzy razy więcej opadów atmosferycznych w najwilgotniejszych miesiącach zimowych w porównaniu z suchym latem.\"\n",
    "italy = \"W epoce żelaza tereny Włoch zamieszkiwali Ligurowie i Sykulowie, a także liczne inne plemiona italskie, celtyckie i iliryjskie. W okresie starożytności tereny Włoch znalazły się pod panowaniem Rzymian. Przed okresem rzymskim tereny Włoch były zamieszkiwane przez Fenicjan i Greków. Od średniowiecza do Risorgimento, pomimo że Półwysep Apeniński był spójny pod względem językowym i kulturowym, jego historia składała się z dziejów niezależnych republik i księstw oraz obcych posiadłości i stref wpływów.\"\n",
    "\n",
    "texts = [orchids, nettles, jokers, bridge, tanks, pistol, pope, knights, george, greece, italy]\n",
    "dox = list(nlp.pipe(texts))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podobieństwo semantyczne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"orchids \", \"nettles\", \"bridge \", \"jokers \", \"tanks \", \"pistol \",\n",
    "          \"pope \", \"knights \", \"george \", \"greece \", \"italy \"]\n",
    "sim_matr = []\n",
    "for x in dox:\n",
    "    sim_matr.append([])\n",
    "    for y in dox:\n",
    "        sim_matr[-1].append(round(x.similarity(y), 3))\n",
    "\n",
    "import numpy\n",
    "arr = numpy.array(sim_matr)\n",
    "doc_sim = pandas.DataFrame(arr, columns=labels, index=labels)\n",
    "print(doc_sim)\n",
    "\n",
    "from itertools import product\n",
    "tokpairs = []\n",
    "for x_i in range(len(dox)):\n",
    "    doc_x = dox[x_i]\n",
    "    for y_i in range(x_i+1, len(dox)):\n",
    "        doc_y = dox[y_i]\n",
    "        tok_prod = product(doc_x, doc_y)\n",
    "        for t1, t2 in tok_prod:\n",
    "            sim = round(t1.similarity(t2), 3)\n",
    "            if sim == 1.0:\n",
    "                continue\n",
    "            tokpairs.append((t1, t2, sim))\n",
    "\n",
    "ranking = sorted(tokpairs, key=lambda x: x[2], reverse=True)\n",
    "        \n",
    "for x in ranking[:50]:\n",
    "    print(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dostęp do wektorów dla dokumentów, tokenów, spanów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orchid_doc = dox[0]\n",
    "doc_vec = orchid_doc.vector\n",
    "tok_vec = orchid_doc[0].vector\n",
    "span_vec = orchid_doc[:3].vector\n",
    "sent_vec = list(orchid_doc.sents)[0].vector\n",
    "print(sent_vec.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flexer\n",
    "\n",
    "Flexer pozwala, w oparciu o Morfeusza, na odmianę pojedynczych wyrazów, a także fraz. Akceptuje więc pojedyncze tokeny, a także ich listy (niekoniecznie ciągłe).\n",
    "\n",
    "Dostęp do niego odbywa się poprzez komponent \"Morfeusz\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(orchid_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "morf_component = nlp.get_pipe(\"morfeusz\")\n",
    "family = orchid_doc[2]\n",
    "target_morph = \"gen:pl\"\n",
    "inflected = morf_component.flex(family, target_morph)\n",
    "print(f\"{family} -({target_morph})-> {inflected}\")\n",
    "\n",
    "phrase = orchid_doc[2:4]\n",
    "inflected = morf_component.flex(phrase, target_morph)\n",
    "print(f\"{phrase} -({target_morph})-> {inflected}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_contiguous = [orchid_doc[32], orchid_doc[41]]\n",
    "target_morph = \"inst\"\n",
    "inflected = morf_component.flex(non_contiguous, target_morph)\n",
    "print(f\"{non_contiguous} -({target_morph})-> {inflected}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorytm Flexera jest również do wykorzystania w procesie lematyzacji wyrażeń złożonych:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tanks_doc = dox[4]\n",
    "print(tanks_doc)\n",
    "\n",
    "tanks_phrase = tanks_doc[4:13]\n",
    "lemmatized = morf_component.lemmatize(tanks_phrase)\n",
    "print(\"\\n\")\n",
    "print(f\"{tanks_phrase} -> {lemmatized}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntxt = \"\"\"Informuję, że moim identyfikatorem podatkowym, którym posługuję się w rozliczeniach podatkowych z Urzędem Skarbowym jest: Numer PESEL 75123202570 (Niepotrzebne skreślić). Adres do wskazania w PIT: ul. Krauthofera 18a/27, 61-203 Poznań.\n",
    "2. Pośrednik kredytu hipotecznego: Powszechna Kasa Oszczędności Bank Polski Spółka Akcyjna siedzibą w Warszawie przy ul. Puławskiej 15, 02-515 Warszawa, http://www.pkobp.pl; infolinia 800 302 302. \n",
    "Prezentowany wykaz dokumentów jest wspólny dla: -_ Powszechnej Kasy Oszczędnościowej Banku Polskiego Spółki Akcyjnej z siedzibą w Warszawie przy ul. Puławskiej 15, 02-515 Warszawa, zwanej dalej „PKO Bank Polski SA”, - PKO Banku Hipotecznego Spółki Akcyjnej z siedzibą w Gdyni przy ul. Jerzego Waszyngtona 17, 81-342 Gdynia, zwanej dalej „PKO Bank Hipoteczny SA”.\n",
    "\\\n",
    "\"\n",
    "\n",
    "Wskazówki dla lekarza kierującego: 1. w zakresie diagnostyki: brak 2; w zakresie farmakoterapii: zyx,entom; 3. inne: brak; Anna Sosnowska - Specjalista gardła, foniatra 160-288 Poznań, l. Promienista 347. \n",
    "\n",
    "\"\n",
    "\"\"\"\n",
    "doc = nlp(ntxt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NASK",
   "language": "python",
   "name": "nask"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
